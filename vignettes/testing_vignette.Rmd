---
title: "Testing Vignette"
author: "Gilberto Padilla Mercado"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse, quietly = TRUE)
devtools::load_all()
tidyverse_conflicts()
```

# Introduction and Instructions

This vignette is designed for a code review and testing of _invivopkfit_.
The central goal is simple: fit a subset of the `cvt` data, evaluate the fits
and derived toxicokinetic statistics.
Furthermore, you should be able to plot the results of the fits.
First, let us get the subset of the `cvt` database that you'll analyze.

```{r cvt_subset}
# set.seed()
chemical_set <- sample(unique(cvt$chemicals_analyzed.dsstox_substance_id),
                       size = 3)

my_predata <- cvt %>%
  dplyr::filter(chemicals_analyzed.dsstox_substance_id %in% union("DTXSID2039336",
                                                                  chemical_set))
```

To ensure that invivopkfit is treating un-fittable objects correctly, I have
suggested the inclusion of **DTXSID2039336** which should yield an optimizer
convergence code of _-9999_.
You will also notice that I have used the function `sample()` to obtain a random
set of 3 chemicals. If you would like to rerun the analysis with the same chemicals,
please un-comment `set.seed()` and add a numeric value of choice in that function
_before_ running the sampling function.


# Fitting the data

Next the data needs to be processed and fit.
When creating a PK object, there are several options that dictate how the models
are fit, and which models and optimizer paradigms to use.
Feel free to change these options at will, to ensure _invivopkfit_'s intended
functions are working.

```{r fit_pipeline}
my_pk <- pk(data = my_predata) +
  facet_data(vars(Chemical, Species)) +
  settings_preprocess(keep_data_original = TRUE) +
  settings_optimx(method = c("L-BFGS-B", "bobyqa")) +
  scale_conc(dose_norm = FALSE, log10_trans = FALSE) +
  scale_time() +
  stat_model() +
  stat_error_model(error_group = vars(Chemical, Species, Reference))

my_pk <- do_preprocess(my_pk)

my_pk <- do_data_info(my_pk)

my_pk <- do_prefit(my_pk)

# This will call the previous steps, so it is possible to run this after pk()
my_pk <- do_fit(my_pk) 
```


```{r get_mehods}
# Add `%>% View` to see the output outside the console
# or save to an object using `<-`
get_status(my_pk)

get_data(my_pk) 
# If no error, continue below
# This should randomly sample 60% of the data and check variables
check_newdata(newdata = get_data(my_pk) %>%
                slice_sample(prop = 0.6),
              olddata = get_data(my_pk),
              req_vars = c("Conc_trans", "Time", "Dose"))
# If TRUE is return, continue please
get_fit(my_pk)

get_prefit(my_pk)

get_nca(my_pk)

get_scale_conc(my_pk)

get_scale_time(my_pk)

get_settings_data_info(my_pk)

get_settings_optimx(my_pk)

get_settings_preprocess(my_pk)

get_stat_error_model(my_pk)

get_stat_model(my_pk)

```

```{r derived_statistics}

# I think these functions may be superfluous with compare_models()
# which I did not update to work with batch mode
logLik(my_pk)
get_winning_model(my_pk,
                  criterion = "AIC")
AIC(my_pk)
BIC(my_pk)


# Derived statistics
get_tkstats(my_pk)
eval_tkstats(my_pk)

coef(my_pk)
coef_sd(my_pk, table_format = TRUE)
unlist(coef_sd(my_pk)$alerts)

fold_errors(my_pk)
rmse(my_pk)
rsq(my_pk)





```

