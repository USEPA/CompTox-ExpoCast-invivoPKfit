---
title: "main_vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{main_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

```

```{r setup}
devtools::load_all()
```


# Introduction

`invivoPKfit` is an R package to automate fitting pharmacokinetic/toxicokinetic models to measured concentration vs. time toxicokinetic data.

This version of the package takes an "object-oriented" approach to this task.

This version of the package is heavily modeled after `ggplot2`, and aims to provide a "grammar of PK modeling."

The basic unit of `ggplot2` is a `ggplot` object. The basic unit of `invivopkfit` is a `pk` object.

A `ggplot2` object is essentially a data set with various instructions about how to visualize that data set. Similarly, a `pk` object is essentially a data set with various instructions about how to fit PK models to that data set.

You provide a set of instructions for building a `ggplot2` plot by adding `geom`, `stat`, `scale`, etc. commands to a `ggplot2` object (like `ggplot(my_data, aes(x=x, y=y)) + geom_point()`). Similarly, you provide a set of instructions for fitting an `invivopkfit` PK model by adding `settings`, `stat`, `scale` commands to a `pk` object.

In `ggplot2`, you can add the instructions in any order. You can overwrite old instructions by adding new ones, if you change your mind. The package will internally re-order the instructions in the correct order to build the plot. `invivopkfit` works exactly the same way.

And just as `ggplot2` doesn't actually build the plot until you issue a `print` command, `invivoPKfit` doesn't actually fit the model until you issue  a `fit` command.


This following few chunks demonstrate the differences in setup between the _pooled_,
_hierarchical/joint_, and _separate_ error modelling fitting options.
It will also show the various other fitting options that scale concentration
and time values.

```{r mininal-pk, eval=FALSE}
cvt_df <- subset(cvt, analyte_dtxsid %in% c("DTXSID8031865",
                                            "DTXSID0020442"))

minimal_pk <- pk(data = cvt_df)
```


# "Joint" analysis
This type of analysis fits each Chemical and Species data group but uses the
standard deviations from each Chemical, Species, Reference group to model error.


## Test preprocessing data

```{r joint_preprocesing, eval=FALSE}
joint_pk <- minimal_pk +
  facet_data(ggplot2::vars(Chemical, Species)) +
  settings_preprocess(keep_data_original = FALSE,
                      suppress.messages = FALSE) +
  settings_optimx(method = c("L-BFGS-B", "bobyqa")) +
  scale_conc(dose_norm = FALSE, log10_trans = TRUE) +
  stat_error_model(error_group = vars(Chemical, Species, Reference)) 

joint_pk <- do_preprocess(joint_pk)
joint_pk <- do_fit(joint_pk)
```


# "Pooled" analysis
This analysis takes all the Chemical and Species data together and models
the error of all the data during fitting.

```{r, eval=FALSE}
pooled_pk <- minimal_pk +
  facet_data(ggplot2::vars(Chemical, Species)) +
  settings_preprocess(keep_data_original = TRUE,
                      suppress.messages = TRUE) +
  settings_optimx(method = c("L-BFGS-B")) +
  scale_conc(dose_norm = TRUE, log10_trans = FALSE) +
  stat_error_model(error_group = vars(Chemical, Species)) 

pooled_pk <- do_fit(pooled_pk)
```

# "Separate" analysis
This analysis takes all the Chemical, Species, and Reference data individually
and models the error similarly.

```{r, eval=FALSE}
separate_pk <- minimal_pk +
  facet_data(ggplot2::vars(Chemical, Species, Reference)) +
  settings_preprocess(keep_data_original = FALSE,
                      suppress.messages = TRUE) +
  settings_optimx(method = c("L-BFGS-B")) +
  scale_conc(dose_norm = FALSE, log10_trans = FALSE) +
  scale_time(new_units = "auto") +
  stat_error_model(error_group = vars(Chemical, Species, Reference)) 

separate_pk <- do_fit(separate_pk)
```


# A worked example

Here's a basic example.

## Select data to be fit

First, select data for a single chemical and species. This is the dataset that we will fit.

```{r}
#select data for a single chemical and species
my_data <- subset(cvt,
                  analyte_dtxsid %in% "DTXSID1021116" & species %in% "rat")


```

## Initialize a `pk` object

Now, initialize a `pk` object.  This contains the data, along with a set of instructions for how to do the model fitting. Note that none of the instructions have actually been carried out yet. At this point, the `pk` object is just the data and the list of instructions.

```{r}
my_pk <- pk(data = my_data) +
  facet_data(facets = ggplot2::vars(Chemical, Species)) +
  settings_preprocess(suppress.messages = FALSE,
                      keep_data_original = TRUE)
```

The instructions aren't carried out until you explicitly ask for them to be carried out, by calling functions that implement steps in the model fitting workflow.

The model fitting workflow has four steps.

## Model fitting workflow

### Step 1: Pre-processing data

The first is data pre-processing. Here, variable names are harmonized, and the data set is cleaned, filtered, and missing values are imputed when necessary. Instructions for this data pre-processing step are given in `my_pk$settings_preprocess`.
Some common harmonized variable names that will be referenced in this document are:  
  
- Chemical: the DTXSID for the analyte of a CvT experiment  
- Species: the species of the subject in a CvT experiment  
- Route: the route by which the Chemical was administered (iv or oral)  
- Media: the medium which was collected for analysis (blood or plasma)  
  


```{r}
#preprocess data
my_pk <- do_preprocess(my_pk)
```

This step adds the pre-processed data to the `pk` object in a new element named `data`.

### Step 2: Data info
 
Next, some summary information is gathered about the data. These are things like the number of detects/non-detects, broken out by the different routes and media present in the data. This step also includes non-compartmental analysis of the data, to calculate things like the empirical Cmax (maximum concentration), empirical tmax (time of maximum concentration), and empirical AUC (area under the concentration-time curve). Instructions for this "data info" step are given in `my_pk$settings_data_info`.

```{r}
#get summary data info
my_pk <- do_data_info(my_pk)
```

This step adds the data summary info to the `pk` object in a new element named `data_info`.

### Step 3: Pre-fitting

Next, a "pre-fitting" step is performed. This step looks at the data, along with the PK models to be fitted, and defines the following:

- Which of the PK model parameters are to be fitted. For example, if there is no oral data, then parameters that define oral absorption rate and oral bioavailability will not be fitted.
- The lower and upper bounds for the PK model parameters to be fitted, which may be constant or may be derived from the data in some way.
- A starting guess at the value for each PK model parameter to be fitted, which may be constant or may be derived from the data in some way.

```{r}
#pre-fitting (get model parameter bounds & starting values)
my_pk <- do_prefit(my_pk)
```

This step adds the parameter lower/upper bounds and starting guesses to the `pk` object.

### Step 4: Model fitting

Finally, the model fitting can be performed.

```{r}
#model fitting
my_pk <- do_fit(my_pk)
```

This step adds the fitted model parameter values to the `pk` object. If multiple models were fitted, and/or if multiple optimization algorithms ("methods") were used to do the fitting, then one set of fitted model parameter values is added for each model and each optimization algorithm ("method").

### Skip straight to Step 4

Note that you could just skip straight to the last step. If you call `do_fit()` on a newly-initialized `pk` object, all of the preceding steps will be done automatically. The following code will give you exactly the same results as going through each step individually as above.

```{r, eval = FALSE}
my_pk <- pk(data = my_data) +
  settings_preprocess(suppress.messages = FALSE,
                      keep_data_original = TRUE)

my_pk <- do_fit(my_pk)
```


### Post-fitting: Getting information about the fitted models

Once fitting has been completed, then you can use some familiar methods to extract information about the fitted model(s). 

#### Coefficients, residuals, and predictions

For example, you can extract the fitted model parameter values using `coef()`. The result will be a `data.frame`, whose columns include model parameters, and whose rows correspond to the optimization algorithms used for fitting and unique identifiers for each *data_group*, Route, and Media.

```{r}
coef(my_pk)
```

You can extract the residuals using `residuals()` (for the observations in the original data) using `predict()`. The result is a `data.frame`, one row for each fitted model, optimization algorithm, and timepoint-value pair given by `predict()`.

```{r}
my_resids <- residuals(my_pk)
head(my_resids)
```

You can get the predicted values (for the time points, doses, and routes in the original data) using `predict()`. The result is a `data.frame`, one for each fitted model, whose columns contain the predictions from the fits by each optimization algorithm.

```{r}
my_preds <- predict(my_pk)
head(my_preds)
```

If instead you want to get predicted values for new data (new timepoints, doses, or routes), you can do this by providing argument `newdata`, but you need to include the variables for the data group!

```{r}
predict(my_pk, newdata = data.frame(
  Chemical = "DTXSID1021116",
  Species = "rat",
  Time = seq(0, 5, by = 0.5),
  Time.Units = "hours",
  Dose = 1,
  Route = "iv",
  Media = "plasma",
  exclude = FALSE))
```

#### Plots

You can plot the fit using `plot()` (which returns a `ggplot2` plot object):

```{r}
p <- plot(x = my_pk)
print(p$final_plot)
```

By default, the concentration axis is scaled the same way as the data was scaled for fitting. You can specify whether to dose-normalize and/or log-transform the plot y axis using argument `use_scale_conc`:

```{r}
p2 <- plot(my_pk,
     use_scale_conc = TRUE
     )
print(p2$final_plot)
```


#### Model evaluation metrics

You can also get model-evaluation metrics such as log-likelihood:

```{r}
logLik(my_pk)
```

AIC:

```{r}
AIC(my_pk)
```

BIC:

```{r}
BIC(my_pk)
```

Root mean squared error (RMSE):

```{r}
rmse(my_pk)
```

R-squared for predicted vs. observed:

```{r}
rsq(my_pk)
```

#### Data

You can get the pre-processed data:

```{r}
data_proc <- get_data(my_pk)
head(data_proc)
```

And you can get summary information about the pre-processed data -- including non-compartmental analysis results:

```{r}
get_data_info(my_pk)
get_nca(my_pk)
```


#### Toxicokinetic statistics

And finally, you can compute derived toxicokinetic statistics, such as total clearance rate, half-life, steady-state plasma concentration, etc., using `get_tkstats()`. (The result is a `data.frame` of TK stats, one for each model and optimization algorithm, that was fitted.)

```{r}
my_tkstats <- get_tkstats(my_pk)
head(my_tkstats)
```

We'll walk through the various pieces of this example.

# Anatomy of a `pk` object

Let's start at the very beginning (a very good place to start). When you initialize a `pk` object, what does that object actually contain?

Here, I'll initialize a `pk` object with all the default options.

```{r}
my_pk <- pk(data = my_data)
```

This newly-initialized `pk` object is, under the hood, a `list` object with the following elements:

```{r}
names(my_pk)
```

## `my_pk$data_original`

The `data_original` element contains the data set you provided in argument `data`. This is the data set that will be fitted. In this case, it is a subset of CvTdb.

```{r}
head(my_pk$data_original)
```

## `my_pk$mapping`

The `mapping` element specifies how the column names in the original data should be translated to the harmonized/standardized variable names that are used internally to `invivopkfit`. This is analogous to providing an `aes` mapping in `ggplot2` -- and in fact, it uses the same `aes()` function as `ggplot2`!

In `ggplot2`, you would specify which variables in your data map to the standardized "aesthetic" variables `x`, `y`, `color`, `shape`, etc., using a command like this:

```{r, eval = FALSE}
ggplot(data = my_data, 
       aes(x = time_hr,
           y = conc,
           color = dose_level_normalized_corrected,
           shape = as.factor(document_id)
       )
)
```


In `invivopkfit`, you specify which variables in your data map to the standardized variables `Chemical`, `Species`, `Time`, `Dose`, etc., using a command like this:

```{r, eval = FALSE}
pk(data = my_data,
   mapping = ggplot2::aes(
                 Chemical = analyte_dtxsid,
                 Chemical_Name = analyte_name_original,
                 DTXSID = analyte_dtxsid,
                 CASRN = analyte_casrn,
                 Species = species,
                 Reference = document_id,
                 Media = conc_medium_normalized,
                 Route = administration_route_normalized,
                 Dose = dose_level_normalized,
                 Dose.Units = "mg/kg",
                 Subject_ID = subject_id,
                 Series_ID = series_id,
                 Study_ID = study_id,
                 ConcTime_ID = conc_time_id,
                 N_Subjects = n_subjects_in_series,
                 Weight = weight_kg,
                 Weight.Units = "kg",
                 Time = time_hr,
                 Time.Units = "hours",
                 Value = conc,
                 Value.Units = "mg/L",
                 Value_SD = conc_sd_normalized,
                 LOQ = loq
               ))
```

The above command shows the default mapping that will be applied if you don't specify a `mapping` argument when you call `pk`. it's a series of "standardized = original" pairs, where the standardized `invivoPKfit` internal variable name is on the left-hand side, and the original data variable name is on the right-hand side. Here, the original data variable names are all from CvTdb. These are variables appearing in the `invivoPKfit` built-in data object `cvt`, which contains a subset of CvTdb data.

```{r}
names(cvt)
```

### Expressions in `aes()`

Just as in `ggplot2`, you can specify expressions inside `aes`. This lets you specify mappings that are more complicated than just a simple one-to-one variable name change. 

For example, the mapping for `Reference` uses an expression:

```{r, eval = FALSE}
Reference = as.character(
  ifelse(
    is.na(
      documents_reference.id
    ),
    documents_extraction.id,
    documents_reference.id
  )
)

```

This expression says "For the standardized variable `Reference`: First look at the original variable `documents_reference.id`. If that variable is NA, then use the variable `documents_extraction.id`; otherwise, use `documents_reference.id`."

This mapping for `Reference` occurs because of the way CvTdb handles references. In CvTdb, concentration-time data are extracted from a particular document. For example, this may be the PDF of a peer-reviewed publication. The unique ID for this document is the "extraction ID." However, the original reference for the data may be a *different* document; this is the "reference ID." For example, this occurred when Wambaugh et al. 2018 (doi: 10.1093/toxsci/kfy020) collected and published a data set that had originally been published in Cruz et al. 2002 (PMID: 12434508). The "reference ID" refers to Cruz et al. (2002); the "extraction ID" refers to Wambaugh et al. (2018). If the data are instead extracted from their original reference, then "reference ID" is left blank (NA) and only "extraction ID" is used. 

You can use expressions to specify constant values for standardized variables, as well. The default mapping contains items like this:

```{r, eval=FALSE}
Value.Units = "mg/L"
```

This mapping says "For the standardized variable `Value.Units`: Don't take the value from any variable in the original data. Just assign the constant value `"mg/L"`to this variable."

## `my_pk$status`

This is an integer code representing the current status of the `pk` object in the overall `pk` workflow. The workflow steps are numbered like this:

1. Initialize the `pk` object in `pk()`
2. Preprocess data in `do_preprocess()`
3. Get data summary info (including non-compartmental analysis) in `do_data_info()`
4. Do pre-fitting (get parameter bounds & starting values) in `do_prefit()`
5. Do model fitting in `do_fit()

You can get the status of a `pk` object at any time using

```{r}
get_status(my_pk)
```


## `my_pk$settings_preprocess`

This element contains the instructions for pre-processing the data. It is the result of a call to the function `settings_preprocess()` with all the default arguments. In other words, 

```{r, eval = FALSE}

my_pk <- pk(my_data)
```

is the same as

```{r, eval = FALSE}
my_pk <- pk(my_data) + settings_preprocess()
```

Here are the default arguments to `settings_preprocess()`.

```{r}
formals(settings_preprocess)
```

### `routes_keep`

This contains a list of the "allowed" routes of administration. Data will be filtered to keep only observations with routes on this list. The default is `c('oral', 'iv')`. Currently, `invivopkfit` only has models implemented for those two routes. So it is not recommended to change the default unless you know what you are doing.

### `media_keep`
This contains a list of the "allowed" tissue media in which concentrations can be measured/predicted. Data will be filtered to keep only observations with media on this list.The default is `c('blood', 'plasma')`. Currently, `invivoPKfit` only has models implemented for those two media. So it is not recommended to change the default unless you know what you are doing.

### `ratio_conc_dose`

This gives the ratio of the mass units used for concentrations to the mass units used for dose. The default is 1, indicating the same mass units are used in each (in CvTdb, by default, concentrations are mg/L and doses are mg/kg). If, for example, you had data where concentrations were ng/L and doses were mg/kg, you would need to use `ratio_conc_dose = 1e-6`. During data preprocessing, concentrations are multiplied by `ratio_conc_dose` so that the units harmonize with dose units.

### `impute_loq`, `loq_group` and `calc_loq_factor`

This instructs `invivopkfit` on whether and how to try to impute values for limits of quantification, if LOQ values are missing for any observations. If `impute_loq = TRUE`, then missing LOQs will be imputed as follows. The data will be split into groups according to unique combinations of the variables specified in `loq_group`. Then, the minimum detected value will be multiplied by `calc_loq_factor`. The result will be imputed for any missing LOQs in that group.

### `impute_sd`, `sd_group`

This instructs `invivopkfit` on whether and how to try to impute missing values for sample standard deviations of observed concentrations. If `impute_sd = TRUE`, then missing SDs will be imputed as follows. The data will be split into groups according to unique combinations of the variables specified in `sd_group`. If there are any non-missing SDs in a group, then missing SDs will be imputed as the minimum non-missing SD in the group. If all SDs in a group are missing, then they will be imputed with 0.


## `my_pk$settings_data_info`

This element provides control settings for getting summary data information, including non-compartmental analysis. It is the result of a call to `settings_data_info()` with default arguments. In other words, just calling `pk()` by itself is the same as

```{r}
my_pk <- pk(my_data) +
  settings_preprocess() +
  settings_data_info()
```

Here are the default arguments to `settings_data_info()`:

```{r}
formals(settings_data_info)
```

### `nca_group` 

This argument instructs `invivopkfit` on how to perform non-compartmental analysis. Data will be split into groups according to unique combinations of the variables specified in `nca_group`. Then, non-compartmental analysis will be performed for each group using `PK::nca()`. 

For each NCA group, a few basic summary statistics will also be computed:

- number of observations  
- number of detects  
- time of first and last observations  
- time of first and last detected observations  

## `my_pk$optimx_settings`

This element provides control settings for the optimizer used to fit the model(s) to the data, which is `optimx::optimx()`. It is the result of a call to `settings_optimx()` with default arguments. In other words, just calling `pk()` by itself is the same as

```{r, eval = FALSE}
my_pk <- pk(my_data) +
  settings_preprocess() +
  settings_data_info() +
  settings_optimx()
```

Here are the default arguments to `settings_optimx()`:

```{r}
formals(settings_optimx)
```

These are simply the arguments for `optimx::optimx()`: `method`, `itnmax`, `hessian`, `control`, and `...`. 

## `my_pk$scales`

This element is itself a list with elements `conc` and `time`, providing instructions on how to scale and/or transform concentration and time variables before fitting any models.

### `my_pk$scales$conc`

The `conc` element is a result of a call to `scales_conc()` with the default arguments. In other words, `pk()` alone is the same as

```{r, eval = FALSE}
my_pk <- pk(my_data) +
  settings_preprocess() +
  settings_data_info() +
  settings_optimx() +
  scale_conc()
```

Here are the default arguments to `scale_conc()`:

```{r}
formals(scale_conc)
```

#### `dose_norm`

This instructs `invivopkfit` on whether to apply dose normalization before fitting -- i.e., divide each observed concentration (and its corresponding observed standard deviation and/or LOQ, if any) by its corresponding dose. Dose normalization may be useful to normalize the residual error if the residual error is heteroscedastic and scales with concentration (because higher dose usually means higher concentration).

#### `log10_trans`

This instructs `invivopkfit` whether to apply a `log10()` transformation to observed concentrations (and LOQs) before fitting. This transformation might also be useful to normalize heteroscedastic residual errors that scale multiplicatively with concentration.

### `my_pk$scales$time`

The `time` element is a result of a call to `scales_time()` with the default arguments (there is only one). Calling `pk()` by itself is the same as

```{r, eval = FALSE}
my_pk <- pk(my_data) +
  settings_preprocess() +
  settings_data_info() +
  settings_optimx() +
  scale_conc() +
  scale_time()
```

Here are the default arguments to `scale_time()`:

```{r}
formals(scale_time)
```

`new_units` instructs `invivokpfit` whether and how to convert observed times into different units. The default `new_units = "identity"` tells `invivopkfit` not to apply any transformations to the time units -- retain the original units. Another useful option is `new_units = "auto"`, which tells `invivopkfit` to choose new units based on the midpoint of observed times -- it will automatically choose time units that put the midpoint time on the order of 10, or as close as it can get to that. You can also specify any time units understood by `lubridate::duration()`, e.g. `minutes`, `hours`, `days`, `weeks`, `months`, `years`, to convert time into those units.

## `my_pk$stat_model`

This is a named list, with one item named for each model to be fitted. It is the result of a call to `stat_model()` with its default arguments. In other words, calling `pk()` by itself is the same as calling

```{r, eval = FALSE}
my_pk <- pk(my_data) +
  settings_preprocess() +
  settings_data_info() +
  settings_optimx() +
  scale_conc() +
  scale_time() +
  stat_model()
```

Here are the default arguments to `stat_model`:

```{r}
formals(stat_model)
```

The `model` argument is a character vector (or scalar), listing the names of one or more models to fit to the data. The default value lists the three models that are already implemented and built in to `invivopkfit`. 

Each model is an object of class `pk_model`, which is simply a named list giving:

- `name`: The model name  
- `params`: The model parameter names (listed as a character vector)  
- `conc_fun`: The name of a function that computes concentrations, given model parameters, time, dose, route, and media  
- `auc_fun`: The name of a function that computes AUC (area under the concentration time curve), given model parameters, time, dose, route, and media  
- `params_fun`" The name of a function that returns bounds and starting points for each of the model parameters, given the data  
- `tkstats_fun`: The name of a function that computes derived TK statistics (such as halflife, clearance rate, etc.) given model parameters  
- `conc_fun_args`: Any additional arguments to the function in `conc_fun`  
- `auc_fun_args`: Any additional arguments to the function in `auc_fun`  
- `params_fun_args`: Any additional arguments to the function in `params_fun`  
- `tkstats_fun_args` : Any additional arguments to the function in `tkstats_fun`  

For example, here is the built-in model `model_1comp`:

```{r}
`model_1comp`
```

You can define a new model by first writing each of the required functions above for that model: a concentration function, an AUC function, a parameters function, and a TK-stats function. Source the R scripts containing those functions. Then use the command `pk_model()`, giving the names of the new functions you have written. Assign the result to an R variable, which will contain your new `pk_model` object. That new model object will persist for the duration of your R session (unless you remove it), and will need to be re-defined if you restart R.

## `my_pk$stat_error_model`

This is the result of a call to `stat_error_model()` with its default arguments.

In other words, calling `pk()` by itself is the same as calling

```{r, eval = FALSE}
my_pk <-pk(my_data) +
  settings_preprocess() +
  settings_data_info() +
  settings_optimx() +
  scale_conc() +
  scale_time() +
  stat_model() +
  stat_error_model()
```

Here are the default arguments to `stat_error_model()`:

```{r}
formals(stat_error_model)
```

There is only one default argument: `error_group`. This specifies the error model as, essentially, a fixed-effects model. The data are split into groups according to unique combinations of the variables specified in `error_group`. Then, for observation $j$ within group $i$, the residual errors are independent and identically distributed as:

$$ c_{ij} = f(t_{ij}, d_{ij}, r_{ij}, m_{ij}; \theta) + \textrm{Normal}(\mu = 0, \sigma = \sigma_{i} ) $$

where for observation $j$ within group $i$:

- $c_{ij}$ is the observed concentration  
- $t_{ij}$ is the time for the observed concentration  
- $d_{ij}$ is the dose for the observed concentration  
- $r_{ij}$ is the dose administration route for the observed concentration  
- $m_{ij}$ is the media (tissue) for the observed concentration  
- $f(t, d; \theta)$ is the model function that predicts concentration given time, dose, route, media, and a vector of model parameter values  
- $\theta$ is the vector of model parameters  

All groups $i = 1, 2, \ldots, N_i$ share the same vector of model parameters $\theta$, but each different group has its own error standard deviation $\sigma_i$. 

The error standard deviations $\sigma_1, \sigma_2, \ldots, \sigma_{N_i}$ are hyperparameters of the model. They will be estimated from the data simultaneously with the model parameters in $\theta$.

The argument `error_group` defines the different groups, and therefore the numnber of error SD hyperparameters that need to be estimated.

If the variables in `error_group` put all the data into the same group, then that simply means there is only one group $i = 1$, and therefore only one error standard deviation $\sigma_1$.

# Providing new instructions for a `pk` object

If you just want to use the default fitting instructions, you can simply do

```{r, eval = FALSE}
my_pk <- pk(data = my_data)
my_pk <- do_fit(my_pk)
```

But if you would like to specify different instructions, you do that by adding settings, scales, and stats using `+`, similarly to the way you add layers in `ggplot2`.

It doesn't matter what order you add the instructions in. 

Here is an example. Notice that I've added the instructions in a different order than shown in the "anatomy of a `pk` object`. That's to emphasize that it doesn't matter what order you add the instructions. Any instructions you don't add explicitly will take their default values.

Note that it will throw messages about replacing existing instructions. These are just messages for your information -- they do not mean anything is wrong!

```{r}
my_pk <- pk(my_data) +
    #instructions to use an error model that puts all observations in the same group
  stat_error_model(error_group = ggplot2::vars(Chemical, Species)) +
  #instructions for concentration scaling/transformation
  scale_conc(dose_norm = TRUE,
             log10_trans = TRUE) +
  #instructions for time rescaling
  scale_time(new_units = "auto") +
  #instructions to use only one method for optimx::optimx()
  settings_optimx(method = "L-BFGS-B") +
  #instructions to impute missing LOQs slightly differently
  settings_preprocess(calc_loq_factor = 0.5) 
```

# Checking the current instructions for a `pk` object

## Check original data

```{r}
get_data_original(my_pk)
```
## Check mapping

```{r}
get_mapping(my_pk)
```

## Check status

```{r}
get_status(my_pk)
```


You can check the current instructions for a `pk` object using various functions that are named `get_[element]`. This is (hopefully) easier than remembering how to access all the list elements in the `pk` object.

## Check `settings_preprocess`

```{r}
get_settings_preprocess(my_pk)
```

## Check `settings_data_info`

```{r}
get_settings_data_info(my_pk)
```

## Check `settings_optimx`

```{r}
get_settings_optimx(my_pk)
```

## Check `scale_conc`

```{r}
get_scale_conc(my_pk)
```

## Check `scale_time`
```{r}
get_scale_time(my_pk)
```

## Check `stat_model`

```{r}
get_stat_model(my_pk)
```

## Check `stat_error_model`

```{r}
get_stat_error_model(my_pk)
```

## Replacing instructions with new ones

You can then overwrite any instructions by adding new/different ones. Let's say you made a mistake -- you actually wanted to dose-normalize concentrations, but *not* log-transform them. 

You can just do this:

```{r}
my_pk <- my_pk +
  scale_conc(dose_norm = TRUE, log10_trans = FALSE)
```

Now the new instructions have replaced the old ones.

```{r}
get_scale_conc(my_pk)
```


And let's say you only want to fit the 1-compartment model -- you don't want the default fits to "model_flat" and "model_2comp". You can do this:

```{r}
my_pk <- my_pk + stat_model(model = "model_1comp")
```

Now, "model_flat" and "model_2comp" models have been removed from the list of models to fit.

```{r}
get_stat_model(my_pk)
```

# What happens to a `pk` object as you go through the steps of model fitting workflow

## Pre-processing data

```{r}
my_pk <- do_preprocess(my_pk)
```

Re-check the names of the `pk` object:

```{r}
names(my_pk)
```

Notice that a new element `data` has been added to the end. This contains the pre-processed data. You can access it using the function `get_data()`.

```{r}
get_data(my_pk)
```

Notice that the variables `Value` and `LOQ` have been translated into `Conc` and `Detect`. If `Value < LOQ` then `Detect = FALSE`; otherwise `Detect = TRUE`. If `Detect = FALSE`, then `Conc = LOQ`; otherwise, `Conc = Value`. This is just a different way of presenting the same information, but it's a bit easier to handle with some of the internal calculations of `invivoPKfit`.

Also, notice that the requested time and concentration transformations have been applied -- compare `Time` and `Time.Units` vs. `Time_trans` and `Time_trans.Units`; compare `Conc` and `Conc.Units` vs. `Conc_trans` and `Conc_trans.Units`.

Notice that missing `LOQ` values have been imputed; compare `LOQ.orig` to `LOQ`. Similarly, missing `Value_SD` values have been imputed; compare `Value_SD.orig` and `Value_SD`. Note that the standard deviations are put in a new variable `Conc_SD` to match with the `Conc` variable. Also, note that the concentration transformations have been applied to the `Conc_SD` variable as well -- see `Conc_SD_trans`. That way, the transformation concentration standard deviations are in the same units as the transformed concentrations.

(A warning: If you do `scale_conc(log10_trans = TRUE)`, then the log10 transformation will be applied to the concentration SD as well as the concentration. This means you can no longer take `Conc_trans + Conc_SD` or `Conc_trans - Conc_SD` as upper and lower bounds. This math is handled correctly internal to `invivopkfit`, but if you extract the pre-processed data and use it for another purpose, be aware of that caveat!)

## Data summary info

```{r}
my_pk <- do_data_info(my_pk)
```

```{r}
names(my_pk)
```

A new element `data_info` has been added. This element is itself a named list containing element "data_summary" and "nca". You can extract these elements using the following functions:

```{r}
get_data_info(my_pk)
```

```{r}
get_nca(my_pk)
```

## Pre-fitting

```{r}
my_pk <- do_prefit(my_pk)
```

```{r}
names(my_pk)
```

No new elements have been added to the top level of `my_pk`. The results of pre-fitting are stored as new sub-elements of `my_pk`.

```{r}
names(my_pk$stat_error_model)
```

```{r}
names(my_pk$stat_model)
```

# Fitting


## Resetting status

Note: Status can be "reset".  -- and then you do something like change the concentration scaling, which changes everything from pre-processing the data (step 2) and downstream -- then the object status will be reset back to 1. This tells `invivopkfit` that it needs to re-do all the steps starting from pre-processing the data.

Let's say I've created a `pk` object. I don't apply any scalings/transformations to concentration or time.

The status of the newly-created `pk` object is 1.

```{r}
my_pk <- pk(data = my_data) + #initialize a `pk` object
  stat_model(model = c("model_flat",
                       "model_1comp",
                       "model_2comp")) + #add PK models to fit
  settings_optimx(method = "L-BFGS-B") #use only this optimx::optimx() algorithm

get_status(my_pk) #status is 1
```

Now I do all the steps up to step 5. (If you just call `do_fit()`, it will fast-forward through all the steps.)

```{r}
my_pk <- do_fit(my_pk)
get_status(my_pk)
```

I can now extract the coefficients and the TK stats for my fitted models.

```{r}
coef(my_pk) 
get_tkstats(my_pk)
```


But now I realize that I should have dose-normalized the concentration. I do this by adding a `scale_conc()`. `invivoPKfit` throws a warning to tell me that the status will be reset back to 1.

```{r}
my_pk <- my_pk + scale_conc(dose_norm = TRUE)
```

And in fact, the status is now reset to 1.

```{r}
get_status(my_pk)
```

This means that I can't do anything that requires a fully fitted model -- not until I re-do all the workflow steps.

```{r, error = TRUE}
coef(my_pk) #throws an error
get_tkstats(my_pk) #throws an error
```

But after I re-fit the new model (setting the status back to 5), I can now extract the new coefficients and TK stats.
```{r}
my_pk <- do_preprocess(my_pk)
my_pk <- do_fit(my_pk)
get_status(my_pk)

coef(my_pk) 
get_tkstats(my_pk)
```

